{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pycle demo 0: a minimal working example example\n",
    "This notebook provides a quick of Compressive Learning with the `pycle` toolbox. It demonstrates the typical workflow with `pycle` and core features. In particular, we will:\n",
    "0. Generate a synthetic dataset\n",
    "1. Sketch it, by first generating our sketch operator\n",
    "2. Use this sketch to estimate the centroids associated with the dataset\n",
    "\n",
    "We assume that you have installed `pycle` with `pip` or that you have the `pycle` folder in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fix the random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 0: generate a dataset with utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the toolbox, we generate a simple synthetic dataset, from a Gaussian mixture model in two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycle.utils.datasets import generatedataset_GMM\n",
    "\n",
    "d = 2     # Dimension\n",
    "K = 4     # Number of Gaussians\n",
    "n = 20000 # Number of samples we want to generate\n",
    "# We use the generatedataset_GMM method from pycle (we ask that the entries are <= 1, and imbalanced clusters)\n",
    "X = generatedataset_GMM(d,K,n,normalize='l_inf-unit-ball',balanced=False) \n",
    "\n",
    "# Bounds on the dataset, necessary for compressive k-means\n",
    "bounds = np.array([-np.ones(d),np.ones(d)]) # We assumed the data is normalized between -1 and 1\n",
    "\n",
    "# Visualize the dataset\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.title(\"Full dataset\")\n",
    "plt.scatter(X[:,0],X[:,1],s=1, alpha=0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: sketching the dataset\n",
    "We first compress the dataset as a sketch. All the tools required for this step are located in the `pycle.sketching` submodule (which we import under the handier alias `sk`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycle.sketching.frequency_sampling import drawFrequencies\n",
    "from pycle.sketching.feature_maps.MatrixFeatureMap import MatrixFeatureMap\n",
    "from pycle.sketching import computeSketch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the sketch of a dataset $X$ is given by\n",
    "\n",
    "$$ \\boldsymbol z = \\sum_{\\boldsymbol x_i \\in X} \\Phi(\\boldsymbol x_i) $$\n",
    "\n",
    "We first generate the feature map $\\Phi$. In this simple example, we chose here random Fourier features $\\Phi(\\boldsymbol x) = \\exp(\\mathrm{i}\\Omega^T\\boldsymbol x)$, which is the usual choice for k-means and GMM fitting. In this expression, $\\Omega$ is a matrix of $m$ \"frequency\" vectors, generated from $\\boldsymbol \\omega_j \\sim \\Lambda$ (without entering into the details, in this case we pick a Folded Gaussian distribution parametrized by $\\Sigma$, which should roughly represent the covariance of the desired clusters). The sketch size $m$ should here be of the order of the number of parameters to estimate, here the number of centroid coordinates, $Kd$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the dimension m: 5*K*d is usually (just) enough in clustering (here m = 50)\n",
    "m = 10*K*d \n",
    "\n",
    "# For this simple example, assume we have a priori a rough idea of the size of the clusters\n",
    "Sigma = 0.1*np.eye(d)\n",
    "\n",
    "# According to the Folded Gaussian rule, we want m frequencies in dimension d, parametrized by Sigma\n",
    "Omega = drawFrequencies(\"FoldedGaussian\",d,m,Sigma)\n",
    "\n",
    "# The feature map is a standard one, the complex exponential of projections on Omega^T\n",
    "Phi = MatrixFeatureMap(\"ComplexExponential\",Omega)\n",
    "\n",
    "# And sketch X with Phi: we map a 20000x2 dataset -> a 50-dimensional complex vector\n",
    "z = computeSketch(X,Phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: compressive learning from the sketch\n",
    "Now that we have computed `z`, we want to learn from this sketch alone. Algorithms that learn from the sketch are  available in the `pycle.compressive_learning` submodule (which we simply refer to as `cl`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycle.compressive_learning.CLOMP_CKM import CLOMP_CKM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve compressive k-means (CKM) with the CLOMP-R algorithm, we create an instance of `cl.CLOMP_CKM`, to which we specify the feature map, the number of centroids we want, the data bounds, and finally the dataset sketch `z`. We then tell the solver to search for the centroids by calling `fit_once()`, and access the found solution as `current_sol` (which returns a tuple with the relative weigths of the clusters, and then the centroids themselves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the solver object\n",
    "ckm_solver = CLOMP_CKM(Phi,K=K,bounds=bounds,sketch=z)\n",
    "\n",
    "# Launch the CLOMP optimization procedure\n",
    "ckm_solver.fit_once()\n",
    "\n",
    "# Get the solution\n",
    "(weights,centroids) = ckm_solver.current_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the centroids (we re-use the dataset for visual comparison)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.title(\"Compressively learned centroids\")\n",
    "plt.scatter(X[:,0],X[:,1],s=1, alpha=0.15)\n",
    "plt.scatter(centroids[:,0],centroids[:,1],s=1000*weights)\n",
    "plt.legend([\"Data\",\"Centroids\"])\n",
    "plt.show()\n",
    "\n",
    "from pycle.utils.metrics import SSE\n",
    "print(\"SSE: {}\".format(SSE(X,centroids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, the size of the orange centroid markers is proportional to the weights of the centroids.\n",
    "As you can see, thanks to `pycle` only a few lines of code suffice to start doing compressive learning. In the next demos, we show how to use some other features of the toolbox, such as automatically setting $\\Sigma$, solving other tasks (GMM), and a privacy-preservation layer on top of the sketch, and using quantized sketch contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}